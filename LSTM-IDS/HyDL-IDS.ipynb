{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb8a4c3f-22a4-4ea9-aafa-2f7c61dce51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\torye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\torye\\anaconda3\\envs\\GPU\\lib\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "c:\\Users\\torye\\anaconda3\\envs\\GPU\\lib\\site-packages\\IPython\\core\\magics\\osm.py:428: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1423293e-53b8-451a-930b-dba217e8e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "#tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853bce4-7fcd-4563-98e5-7bd97c022f7e",
   "metadata": {},
   "source": [
    "# Attack Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36521b6c-634a-4994-8ec2-60dd18ef1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gear = pd.read_csv('./work/HyDL_dataset/CAN/gear_dataset_HyDL.csv')\n",
    "#df_rpm = pd.read_csv('./work/HyDL_dataset/CAN/RPM_dataset_HyDL.csv')\n",
    "#df_dos = pd.read_csv('./work/HyDL_dataset/CAN/DoS_dataset_HyDL.csv')\n",
    "#df_fuzzy = pd.read_csv('./work/HyDL_dataset/CAN/Fuzzy_dataset_HyDL.csv')\n",
    "\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/HYUNDAI_Sonata/HY_Sonata_Replay_DEC.csv')\n",
    "df = pd.read_csv('./work/Survival_and_Challenge_2019/KIA_Soul/KIA_Soul_Replay_DEC.csv')\n",
    "\n",
    "\n",
    "\n",
    "#df = df.iloc[:100]\n",
    "\n",
    "protocol = \"CAN\"\n",
    "#protocol = \"CAN-FD\"\n",
    "\n",
    "#attack = \"gear\"\n",
    "#attack = \"RPM\"\n",
    "#attack = \"DoS\"\n",
    "#attack = \"Fuzzy\"\n",
    "attack = \"Replay\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc04ba-19be-47e3-90ba-81f6c61f4422",
   "metadata": {},
   "source": [
    "# Make folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e46b4f-508a-40c9-ba63-77e92f53cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version = \"HY_SONATA\"\n",
    "version = \"KIA_SOUL\"\n",
    "#version = \"HY_G80\"\n",
    "\n",
    "folder_name = f\"HyDL_{protocol}_{version}\"\n",
    "\n",
    "directory = \"./Models/22.HyDL_(VC)\" \n",
    "path = os.path.join(directory, folder_name)\n",
    "\n",
    "\n",
    "\n",
    "# Check that the folder already exists and Delete folder if it exists\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path) \n",
    "\n",
    "# Create a new folder\n",
    "os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c80b0-174e-4366-897b-09424268a85e",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a944628-e730-4898-ae2f-d72437473b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "(566338, 10, 1)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#======================================# \n",
    "print(\"Start\")\n",
    "#======================================# \n",
    "\n",
    "if protocol == \"CAN-FD\":\n",
    "    num_data = 64\n",
    "else:\n",
    "    num_data = 8\n",
    "\n",
    "df = df.drop(['Timestamp'], axis=1)\n",
    "arbitration_id_array = df['Arbitration_ID'].to_numpy()\n",
    "\n",
    "dlc_min_val = df['DLC'].min()\n",
    "dlc_max_val = df['DLC'].max()\n",
    "\n",
    "def normalize(val, min_val, max_val):\n",
    "    if max_val == min_val:\n",
    "        return 0\n",
    "    else:\n",
    "        return (np.log(val + 1) - np.log(min_val + 1)) / (np.log(max_val + 1) - np.log(min_val + 1))\n",
    "\n",
    "df['DLC'] = df['DLC'].apply(lambda x: normalize(x, dlc_min_val, dlc_max_val))\n",
    "\n",
    "for i in range(num_data):\n",
    "    column = f'Data{i}'\n",
    "    min_val = df[column].min()\n",
    "    max_val = df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: normalize(x, min_val, max_val))\n",
    "\n",
    "\n",
    "dlc_array = df['DLC'].to_numpy()\n",
    "data_arrays = [df[f'Data{i}'].to_numpy() for i in range(num_data)]\n",
    "\n",
    "arrays_to_concatenate = [arbitration_id_array] + [dlc_array] + data_arrays\n",
    "concatenated_array = np.column_stack(arrays_to_concatenate)\n",
    "\n",
    "if protocol == \"CAN-FD\":\n",
    "    training = concatenated_array.reshape(-1, 66, 1)\n",
    "else:\n",
    "    training = concatenated_array.reshape(-1, 10, 1)\n",
    "\n",
    "print(training.shape)\n",
    "\n",
    "labeling = df['Class'].to_numpy()\n",
    "\n",
    "#======================================# \n",
    "print(\"Done\")\n",
    "#======================================# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d878995-9b5d-461c-85cb-471f06272162",
   "metadata": {},
   "source": [
    "# Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9aa9071-eb8f-4d39-9892-7f3fcc667624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split : Done\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    training, labeling, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "#======================================#  \n",
    "print(\"Dataset Split : Done\")\n",
    "#======================================#  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db9540-b66c-4562-bcfb-221faba3b56b",
   "metadata": {},
   "source": [
    "# HyDL-IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "477901d8-746d-4ea9-a758-d3adaa68dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 10, 1)]           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 8, 32)             128       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 32)            128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 4, 32)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4, 32)             0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 2, 64)             6208      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 2, 64)            256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 1, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               98816     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,177\n",
      "Trainable params: 121,985\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def HyDL_IDS(input_shape, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(32, 3, activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Conv1D(64, 3, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.LSTM(128, activation='tanh')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x) \n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (66, 1) if protocol == \"CAN-FD\" else (10, 1)\n",
    "\n",
    "model = HyDL_IDS(input_shape)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e41db8db-fdf1-4f09-9a4d-5825b469fdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5929/5929 - 53s - loss: 0.5453 - accuracy: 0.7476 - val_loss: 0.4742 - val_accuracy: 0.7802 - 53s/epoch - 9ms/step\n",
      "Epoch 2/10\n",
      "5929/5929 - 49s - loss: 0.4844 - accuracy: 0.7771 - val_loss: 0.4559 - val_accuracy: 0.7921 - 49s/epoch - 8ms/step\n",
      "Epoch 3/10\n",
      "5929/5929 - 49s - loss: 0.4640 - accuracy: 0.7874 - val_loss: 0.4529 - val_accuracy: 0.8010 - 49s/epoch - 8ms/step\n",
      "Epoch 4/10\n",
      "5929/5929 - 49s - loss: 0.4420 - accuracy: 0.7996 - val_loss: 0.3975 - val_accuracy: 0.8205 - 49s/epoch - 8ms/step\n",
      "Epoch 5/10\n",
      "5929/5929 - 49s - loss: 0.4370 - accuracy: 0.8019 - val_loss: 0.4311 - val_accuracy: 0.7968 - 49s/epoch - 8ms/step\n",
      "Epoch 6/10\n",
      "5929/5929 - 49s - loss: 0.4236 - accuracy: 0.8069 - val_loss: 0.3846 - val_accuracy: 0.8160 - 49s/epoch - 8ms/step\n",
      "Epoch 7/10\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "5929/5929 - 49s - loss: 0.4233 - accuracy: 0.8065 - val_loss: 0.4674 - val_accuracy: 0.8025 - 49s/epoch - 8ms/step\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Define a custom callback to log batch losses\n",
    "class BatchLossCSVLogger(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "batch_loss_logger = BatchLossCSVLogger()\n",
    "\n",
    "# Define an EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, verbose=1, mode='max', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=2,\n",
    "                    callbacks=[batch_loss_logger, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2de2a3a5-1edc-4e0e-9018-96a963e23d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch validation losses saved to ./Models/22.HyDL_(VC)\\HyDL_CAN_HY_SONATA\\Replay_batch_losses.csv\n"
     ]
    }
   ],
   "source": [
    "# Write batch losses to a CSV file with a dynamic filename\n",
    "file_path = os.path.join(path, f'{attack}_batch_losses.csv')\n",
    "with open(file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Batch', 'Loss'])\n",
    "    for i, loss in enumerate(batch_loss_logger.losses):\n",
    "        writer.writerow([i, loss])\n",
    "        \n",
    "print(f\"Batch validation losses saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e70360b-a3e5-4b91-994d-b010934d9d79",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80b3b66f-f3ff-49d5-951f-a68e6bc97083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5841/5841 [==============================] - 12s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test, category, results):\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_classes = (predicted_probabilities > 0.5).astype(\"int32\")\n",
    "    accuracy = accuracy_score(y_test, predicted_classes)\n",
    "    precision = precision_score(y_test, predicted_classes)\n",
    "    recall = recall_score(y_test, predicted_classes)\n",
    "    f1 = f1_score(y_test, predicted_classes)\n",
    "\n",
    "    temp_df = pd.DataFrame(\n",
    "        {'Category': [category], 'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1': [f1]})\n",
    "    return pd.concat([results, temp_df], ignore_index=True)\n",
    "\n",
    "\n",
    "test_scenarios = {\n",
    "    attack : {'X_test': X_test, 'y_test': y_test}\n",
    "}\n",
    "\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "for category, scenario in test_scenarios.items():\n",
    "    X_test_id = scenario['X_test']\n",
    "\n",
    "    y_test = scenario['y_test']\n",
    "\t\n",
    "    all_results = evaluate_model(model, X_test, y_test, category, all_results)\n",
    "\n",
    "\n",
    "\n",
    "csv_file_name = f\"{path}/{protocol}_{version}_{attack}.csv\"\n",
    "h5_model_name = f\"{path}/{protocol}_{version}_{attack}_model.h5\"\n",
    "\n",
    "\n",
    "all_results.to_csv(csv_file_name, index=False)\n",
    "model.save(h5_model_name, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5102c55-3afd-405c-a80e-8f583764ef0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0dbd6-d102-4291-ae01-894fb6a712ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030c15f-934c-4041-b1aa-0c5a96b8eeea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1705fba-8bdd-48b6-a99a-3026306921b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54e034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81fc3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bca64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
