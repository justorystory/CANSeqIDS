{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2649a91f-937b-4b08-ac5c-c5226d6c0dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\torye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\torye\\anaconda3\\envs\\GPU\\lib\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "C:\\Users\\torye\\anaconda3\\envs\\GPU\\lib\\site-packages\\IPython\\core\\magics\\osm.py:428: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d879cd40-cd14-40cc-935b-329d76f46264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCanNet (IEEE TVT)\\n1. Adam optimizer (learning rate = 0.0001)\\n2. Epochs = 15\\n3. Activation function output = softmax\\n4. Categorical_crossentropy\\n5. Batch size = 256\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CanNet (IEEE TVT)\n",
    "1. Adam optimizer (learning rate = 0.0001)\n",
    "2. Epochs = 15\n",
    "3. Activation function output = softmax\n",
    "4. Categorical_crossentropy\n",
    "5. Batch size = 256\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d184a9df-fa94-4220-b4f2-22f5c8dd48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "#tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38336a04-5b80-4b51-88d0-045004b78751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.9.1\n",
      "Built with CUDA: True\n",
      "Built with XLA: True\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "CUDA version (TF build): 64_112\n",
      "cuDNN version (TF build): 64_8\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"Built with XLA:\", tf.test.is_built_with_xla())\n",
    "\n",
    "# GPU 인식 확인\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# (TF 2.11+ 권장) 빌드 메타 정보\n",
    "try:\n",
    "    from tensorflow.python.platform import build_info as tf_build_info\n",
    "    print(\"CUDA version (TF build):\", tf_build_info.build_info.get('cuda_version'))\n",
    "    print(\"cuDNN version (TF build):\", tf_build_info.build_info.get('cudnn_version'))\n",
    "except Exception as e:\n",
    "    print(\"build_info not available:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ccd387-ae98-43bb-b719-98913f8d2cc6",
   "metadata": {},
   "source": [
    "# Attack Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a1788e-5454-4d4e-b5b6-094454c29042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('./work/HyDL_dataset/CAN/gear_dataset_HyDL.csv')\n",
    "#df = pd.read_csv('./work/HyDL_dataset/CAN/RPM_dataset_HyDL.csv')\n",
    "#df = pd.read_csv('./work/HyDL_dataset/CAN/DoS_dataset_HyDL.csv')\n",
    "#df = pd.read_csv('./work/HyDL_dataset/CAN/Fuzzy_dataset_HyDL.csv')\n",
    "\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/HYUNDAI_Sonata/HY_Sonata_Malfunction_DEC.csv')\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/HYUNDAI_Sonata/HY_Sonata_Flooding_DEC.csv')\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/HYUNDAI_Sonata/HY_Sonata_Fuzzy_DEC.csv')\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/HYUNDAI_Sonata/HY_Sonata_Replay_DEC.csv')\n",
    "\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/KIA_Soul/KIA_Soul_Malfunction_DEC.csv')\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/KIA_Soul/KIA_Soul_Flooding_DEC.csv')\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/KIA_Soul/KIA_Soul_Fuzzy_DEC.csv')\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/KIA_Soul/KIA_Soul_Replay_DEC.csv')\n",
    "\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/CHEVROLET_Spark/CHEVROLET_Spark_Malfunction_DEC.csv')\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/CHEVROLET_Spark/CHEVROLET_Spark_Flooding_DEC.csv')\n",
    "#df = pd.read_csv('./work/Survival_and_Challenge_2019/CHEVROLET_Spark/CHEVROLET_Spark_Fuzzy_DEC.csv')\n",
    "\n",
    "\n",
    "#df = pd.read_csv('./work/HyDL_dataset/CAN-FD/CANFD_Malfunction_HyDL.csv')\n",
    "#df = pd.read_csv('./work/HyDL_dataset/CAN-FD/CANFD_Flooding_HyDL.csv')\n",
    "#df = pd.read_csv('./work/HyDL_dataset/CAN-FD/CANFD_Fuzzing_HyDL.csv')\n",
    "\n",
    "#df = df.iloc[:100000]\n",
    "\n",
    "df1 = pd.read_csv('./work/UAVCAN_Attack_dataset/type5_DEC.csv')\n",
    "df2 = pd.read_csv('./work/UAVCAN_Attack_dataset/type6_DEC.csv')\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "\n",
    "#protocol = \"CAN\"\n",
    "#protocol = \"CAN-FD\"\n",
    "protocol = \"UAVCAN\"\n",
    "\n",
    "#attack = \"gear\"\n",
    "#attack = \"RPM\"\n",
    "#attack = \"DoS\"\n",
    "#attack = \"Fuzzy\"\n",
    "#attack = \"Replay\"\n",
    "\n",
    "#attack = \"Malfunction\"\n",
    "#attack = \"Flooding\"\n",
    "#attack = \"Fuzzing\"\n",
    "\n",
    "#attack = \"Flooding\"\n",
    "#attack = \"Fuzzy\"\n",
    "attack = \"Replay\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ac41e-f559-4bdb-a3f2-365b1487b06f",
   "metadata": {},
   "source": [
    "# Make Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d0c5d3-22d0-45e3-b334-b2ea9d41e367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Check that the folder already exists and Delete folder if it exists\\nif os.path.exists(path):\\n    shutil.rmtree(path) \\n\\n# Create a new folder\\nos.makedirs(path)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version = \"HY_SONATA\"\n",
    "#version = \"KIA_SOUL\"\n",
    "#version = \"CHEVROLET_SPARK\"\n",
    "#version = \"HY_G80\"\n",
    "version = \"UAV\"\n",
    "\n",
    "folder_name = f\"CanNet_{protocol}_{version}\"\n",
    "\n",
    "directory = \"./Models/24.CanNet_(TVT)\" \n",
    "path = os.path.join(directory, folder_name)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Check that the folder already exists and Delete folder if it exists\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path) \n",
    "\n",
    "# Create a new folder\n",
    "os.makedirs(path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02477f0-437e-4cb6-bc00-5325f56df3d0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb464b01-7dd2-4213-a3de-f5f019b90b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0% (0/26362)\n",
      "Progress: 10% (2636/26362)\n",
      "Progress: 20% (5272/26362)\n",
      "Progress: 30% (7908/26362)\n",
      "Progress: 40% (10544/26362)\n",
      "Progress: 50% (13180/26362)\n",
      "Progress: 60% (15816/26362)\n",
      "Progress: 70% (18452/26362)\n",
      "Progress: 80% (21088/26362)\n",
      "Progress: 90% (23724/26362)\n",
      "Progress: 100% (26360/26362)\n",
      "Progress: 100% (Completed)\n"
     ]
    }
   ],
   "source": [
    "#======================================#  \n",
    "#   RGB\n",
    "#======================================#  \n",
    "def can_id_to_rgb(can_id):\n",
    "    hex_value = hex(int(can_id))[2:].upper().zfill(3)\n",
    "    R = 255 - int(hex_value[0], 16) * 17\n",
    "    G = 255 - int(hex_value[1], 16) * 17\n",
    "    B = 255 - int(hex_value[2], 16) * 17\n",
    "    return (R, G, B)\n",
    "\n",
    "\n",
    "#======================================#  \n",
    "#   Image resizing and normalization\n",
    "#======================================#  \n",
    "def preprocess_image(image):\n",
    "    image = tf.image.resize(image, [64, 64])    # Resize to 64x64\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # # Normalize to 0~1          =>정규화 안해도 되는지 성능으로 확인할\n",
    "    return image\n",
    "        \n",
    "\n",
    "#======================================#  \n",
    "#   Generate Images\n",
    "#======================================#  \n",
    "def data_to_image(data_path):\n",
    "    df['Arbitration_ID'] = df['Arbitration_ID'].astype(int)\n",
    "    unique_timestamps = np.unique(df['Timestamp'])\n",
    "    timestamp_groups = [unique_timestamps[i:i + 16] for i in range(0, len(unique_timestamps), 16)]\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    total_groups = len(timestamp_groups)\n",
    "\n",
    "    for group_index, timestamps in enumerate(timestamp_groups):\n",
    "        if group_index % (total_groups // 10) == 0: \n",
    "            progress = (group_index / total_groups) * 100\n",
    "            print(f\"Progress: {progress:.0f}% ({group_index}/{total_groups})\")\n",
    "\n",
    "        image_data = np.zeros((16, 16, 3), dtype=np.uint8)\n",
    "        has_abnormal = False\n",
    "\n",
    "        for i, timestamp in enumerate(timestamps):\n",
    "            if i >= 16:\n",
    "                break\n",
    "            timestamp_data = df[df['Timestamp'] == timestamp]\n",
    "            for index, row in timestamp_data.iterrows():\n",
    "                if 0 < row['DLC'] <= 8:\n",
    "                    rgb_color = can_id_to_rgb(row['Arbitration_ID'])\n",
    "                    position = int(row['DLC'] - 1)\n",
    "                    if position < 16:\n",
    "                        image_data[i, position, :] = rgb_color\n",
    "                    if row['Class'] == 1:\n",
    "                        has_abnormal = True\n",
    "\n",
    "\n",
    "        image_data = preprocess_image(image_data)   # Apply resizing and normalization\n",
    "        images.append(image_data)\n",
    "        labels.append(1 if has_abnormal else 0)\n",
    "\n",
    "    print(\"Progress: 100% (Completed)\")\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "images, labels = data_to_image(df)\n",
    "\n",
    "one_hot_labels = to_categorical(labels, num_classes=2)\n",
    "one_hot_labels = one_hot_labels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9041d-a6a8-4f83-8816-a4518cac429c",
   "metadata": {},
   "source": [
    "# Dataset Split (8:1:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed417f4d-4164-436a-b256-27f16e834e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_temp, y_train, y_test_temp = train_test_split(\n",
    "    images, one_hot_labels, test_size=1/5, random_state=42)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test_temp, y_test_temp, test_size=1/2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61a5d8-69cd-4bae-bfc1-ff2ce52d9e73",
   "metadata": {},
   "source": [
    "# CanNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8f869e4-c390-473d-ab20-02617325b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 32, 16)        448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 3, 3, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 288)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                18496     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,714\n",
      "Trainable params: 23,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def CanNet(input_shape_1, num_classes=2):\n",
    "    input_1 = Input(shape=input_shape_1)\n",
    "    x1 = layers.Conv2D(16, (3, 3), strides=2, padding='same', activation='relu')(input_1)\n",
    "    x1 = layers.Conv2D(32, (3, 3), strides=2, padding='same', activation='relu')(x1)\n",
    "    x1 = layers.MaxPooling2D((8, 8), strides=4, padding='valid')(x1)\n",
    "    x1 = layers.Flatten()(x1)\n",
    "    x1 = layers.Dense(64, activation='relu')(x1)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x1)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_1, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape_1 = (64, 64, 3)  \n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "model = CanNet(input_shape_1, num_classes)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0e52d1b-c681-4525-a7fa-cc7d9d86f5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "83/83 - 4s - loss: 0.5657 - accuracy: 0.7449 - val_loss: 0.5374 - val_accuracy: 0.7641 - 4s/epoch - 52ms/step\n",
      "Epoch 2/15\n",
      "83/83 - 1s - loss: 0.5227 - accuracy: 0.7712 - val_loss: 0.5273 - val_accuracy: 0.7641 - 522ms/epoch - 6ms/step\n",
      "Epoch 3/15\n",
      "83/83 - 1s - loss: 0.5108 - accuracy: 0.7712 - val_loss: 0.5130 - val_accuracy: 0.7641 - 531ms/epoch - 6ms/step\n",
      "Epoch 4/15\n",
      "83/83 - 0s - loss: 0.4939 - accuracy: 0.7718 - val_loss: 0.4947 - val_accuracy: 0.7664 - 490ms/epoch - 6ms/step\n",
      "Epoch 5/15\n",
      "83/83 - 1s - loss: 0.4761 - accuracy: 0.7812 - val_loss: 0.4801 - val_accuracy: 0.7793 - 538ms/epoch - 6ms/step\n",
      "Epoch 6/15\n",
      "83/83 - 1s - loss: 0.4628 - accuracy: 0.7892 - val_loss: 0.4698 - val_accuracy: 0.7922 - 536ms/epoch - 6ms/step\n",
      "Epoch 7/15\n",
      "83/83 - 1s - loss: 0.4534 - accuracy: 0.7971 - val_loss: 0.4620 - val_accuracy: 0.7945 - 554ms/epoch - 7ms/step\n",
      "Epoch 8/15\n",
      "83/83 - 1s - loss: 0.4458 - accuracy: 0.8005 - val_loss: 0.4595 - val_accuracy: 0.7975 - 557ms/epoch - 7ms/step\n",
      "Epoch 9/15\n",
      "83/83 - 1s - loss: 0.4388 - accuracy: 0.8037 - val_loss: 0.4515 - val_accuracy: 0.7998 - 582ms/epoch - 7ms/step\n",
      "Epoch 10/15\n",
      "83/83 - 1s - loss: 0.4333 - accuracy: 0.8063 - val_loss: 0.4455 - val_accuracy: 0.8005 - 587ms/epoch - 7ms/step\n",
      "Epoch 11/15\n",
      "83/83 - 1s - loss: 0.4282 - accuracy: 0.8086 - val_loss: 0.4423 - val_accuracy: 0.8032 - 575ms/epoch - 7ms/step\n",
      "Epoch 12/15\n",
      "83/83 - 1s - loss: 0.4239 - accuracy: 0.8101 - val_loss: 0.4387 - val_accuracy: 0.8039 - 556ms/epoch - 7ms/step\n",
      "Epoch 13/15\n",
      "83/83 - 1s - loss: 0.4213 - accuracy: 0.8108 - val_loss: 0.4362 - val_accuracy: 0.7998 - 560ms/epoch - 7ms/step\n",
      "Epoch 14/15\n",
      "83/83 - 1s - loss: 0.4191 - accuracy: 0.8123 - val_loss: 0.4338 - val_accuracy: 0.8013 - 605ms/epoch - 7ms/step\n",
      "Epoch 15/15\n",
      "83/83 - 1s - loss: 0.4173 - accuracy: 0.8124 - val_loss: 0.4328 - val_accuracy: 0.8020 - 553ms/epoch - 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Custom Callback to log training and validation losses for each batch and epoch\n",
    "class BatchLossCSVLogger(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "batch_loss_logger = BatchLossCSVLogger()\n",
    "\n",
    "\n",
    "# Train the model with the custom callback\n",
    "history = model.fit(\n",
    "        X_train, y_train, epochs=15,\n",
    "        batch_size=256,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=2,\n",
    "        callbacks=batch_loss_logger  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bafaac1-1089-4fdc-a8d3-b6b340aebff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch validation losses saved to ./Models/24.CanNet_(TVT)\\CanNet_UAVCAN_UAV\\Replay_batch_losses.csv\n",
      "Epoch validation losses saved to ./Models/24.CanNet_(TVT)\\CanNet_UAVCAN_UAV\\Replay_epoch_val_losses.csv\n"
     ]
    }
   ],
   "source": [
    "# Write batch losses to a CSV file with a dynamic filename\n",
    "file_path = os.path.join(path, f'{attack}_batch_losses.csv')\n",
    "with open(file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Batch', 'Loss'])\n",
    "    for i, loss in enumerate(batch_loss_logger.losses):\n",
    "        writer.writerow([i, loss])\n",
    "        \n",
    "print(f\"Batch validation losses saved to {file_path}\")\n",
    "\n",
    "\n",
    "# Save epoch validation losses to CSV\n",
    "csv_file_path = os.path.join(path, f'{attack}_epoch_val_losses.csv')\n",
    "with open(csv_file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Epoch', 'Val_Loss'])\n",
    "    for i, val_loss in enumerate(batch_loss_logger.val_losses):\n",
    "        writer.writerow([i, val_loss])\n",
    "\n",
    "print(f\"Epoch validation losses saved to {csv_file_path}\")\n",
    "\n",
    "\n",
    "# Save model\n",
    "h5_model_name = f\"{path}/{attack}_model.h5\"\n",
    "model.save(h5_model_name, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e1ae9-e249-4bc0-8a47-2050e5fc74ff",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75adb703-2abf-40af-871e-f94b39c4f70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step\n",
      "Confusion matrix saved to ./Models/24.CanNet_(TVT)\\CanNet_UAVCAN_UAV/Replay_confusion_matrix.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1\n",
       "0  119   471\n",
       "1   30  2016"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform prediction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Select the class with the highest probability as the predicted value\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame and save it as a CSV file\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix)\n",
    "conf_matrix_file_name = f\"{path}/{attack}_confusion_matrix.csv\"\n",
    "conf_matrix_df.to_csv(conf_matrix_file_name, index=False)\n",
    "print(f\"Confusion matrix saved to {conf_matrix_file_name}\")\n",
    "\n",
    "\"\"\"\n",
    "# Generate a classification report\n",
    "class_report = classification_report(y_true, y_pred_classes, digits=4, output_dict=True)\n",
    "class_report_df = pd.DataFrame(class_report).transpose()\n",
    "class_report_file_name = f\"{path}/{attack}_classification_report.csv\"\n",
    "class_report_df.to_csv(class_report_file_name, index=True)\n",
    "print(f\"Classification report saved to {class_report_file_name}\")\n",
    "\"\"\"\n",
    "\n",
    "conf_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19de7e1-cf2f-4f32-b3e3-c0d6a4f64a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 2016\n",
      "FP: 471\n",
      "FN: 30\n",
      "TN: 119\n"
     ]
    }
   ],
   "source": [
    "# Extract values from the confusion matrix\n",
    "TP = conf_matrix_df.iloc[1, 1]  # True Positive\n",
    "FP = conf_matrix_df.iloc[0, 1]  # False Positive\n",
    "FN = conf_matrix_df.iloc[1, 0]  # False Negative\n",
    "TN = conf_matrix_df.iloc[0, 0]  # True Negative\n",
    "\n",
    "# Print the results\n",
    "print(f\"TP: {TP}\")\n",
    "print(f\"FP: {FP}\")\n",
    "print(f\"FN: {FN}\")\n",
    "print(f\"TN: {TN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebf247e4-5e9d-4747-89e6-cd7704929d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8099\n",
      "Precision: 0.8106\n",
      "Recall: 0.9853\n",
      "F1 Score: 0.8895\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "f1_value = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d6ccc4-c1fc-43cc-aba5-0ab7adfafd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics saved to ./Models/24.CanNet_(TVT)\\CanNet_UAVCAN_UAV/Replay_evaluation_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save calculated evaluation metrics\n",
    "evaluation_metrics = {\n",
    "    \"Accuracy\": [accuracy],\n",
    "    \"Precision\": [precision],\n",
    "    \"Recall\": [recall],\n",
    "    \"F1 Score\": [f1_value]\n",
    "}\n",
    "\n",
    "\n",
    "evaluation_metrics_df = pd.DataFrame(evaluation_metrics)\n",
    "evaluation_metrics_file_name = f\"{path}/{attack}_evaluation_metrics.csv\"\n",
    "evaluation_metrics_df.to_csv(evaluation_metrics_file_name, index=False)\n",
    "\n",
    "\n",
    "print(f\"Evaluation metrics saved to {evaluation_metrics_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4bf56-4f57-485a-a363-8b4620f7d792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b16080-3649-4bf2-b0b5-0d6961ff9d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a3e76-bc58-43f1-b5b2-b8a393410b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
